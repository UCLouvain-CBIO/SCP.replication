---
title: "Replication of the nPOP analysis (Leduc et al. 2022)"
author:
    - Christophe Vanderaa^[christophe.vanderaa@uclouvain.be], Computational Biology, UCLouvain
    - Laurent Gatto, Computational Biology, UCLouvain
output:
    BiocStyle::html_document:
    self_contained: yes
toc: true
toc_float: true
toc_depth: 2
code_folding: show
date: "`r BiocStyle::doc_date()`"
package: "`r BiocStyle::pkg_ver('SCP.replication')`"
vignette: >
    %\VignetteIndexEntry{nPOP replication}
    %\VignetteEngine{knitr::rmarkdown}
    %\VignetteEncoding{UTF-8}
---

```{r setup, include = FALSE}
## Options for Rmarkdown compilation
knitr::opts_chunk$set(fig.width = 7,
                      fig.height = 5,
                      fig.align = "center",
                      out.width = "70%",
                      message = FALSE,
                      collapse = TRUE,
                      crop = NULL ## Related to https://stat.ethz.ch/pipermail/bioc-devel/2020-April/016656.html
)
## Time the compilation
timeStart <- Sys.time()
```

# Introduction

nPOP ([Leduc et al. 2022](http://dx.doi.org/10.1101/2021.04.24.441211))
is an upgrade of the SCoPE2 protocole ([Specht et al. 2021](http://dx.doi.org/10.1186/s13059-021-02267-5) 
and [Petelski et al. 2021](http://dx.doi.org/10.1038/s41596-021-00616-z)), 
where the mPOP sample preparation method is replaced by the nPOP method. nPOP processes
samples using the Cellenion dispensing device and uses DMSO as lysis 
reagent instead of a freeze-thaw procedure. They also include the
prioritized data acquisition mode as described by 
[Huffman et al. 2022](http://dx.doi.org/10.1101/2022.03.16.484655).

Let's first load the replication package to make use of some helper
functions. Those functions are only meant for this replication 
vignette and are not designed for general use. 

```{r, message = FALSE}
library("SCP.replication")
```

### `scp` and the SCoPE2 workflow

The code provided along with the article can be retrieved from
[this GitHub repository](https://github.com/SlavovLab/nPOP).
The objective of this vignette is to replicate the analysis script 
while providing standardized, easy-to-read, and well documented code. 
Therefore, our first contribution is to formalize the data processing
into a conceptual flow chart.

```{r, results='markup', fig.cap="Overview of the processing workflow by Leduc et al.", echo=FALSE, out.width='100%', fig.align='center'}
knitr::include_graphics("leduc2022-workflow.png")
```

This replication vignette relies on a data framework dedicated to SCP
data analysis that combines two Bioconductor classes 
([Vanderaa et al. 2021](http://dx.doi.org/10.1080/14789450.2021.1988571)):

- The `SingleCellExperiment` class provides an interface to many 
cutting edge methods for single-cell analysis 
- The `QFeatures` class facilitates manipulation and processing of
MS-based quantitative data. 

The [`scp` vignette](http://www.bioconductor.org/packages/release/bioc/vignettes/scp/inst/doc/scp.html) 
provides detailed information about the data structure. The `scp` 
package extends the functionality of `QFeatures` for single-cell 
application. `scp` offers a standardized implementation for single-cell
processing methods.

The required packages for running this workflow are listed below.

```{r libraries, results='hide', message=FALSE}
## Core packages of this workflow
library(SingleCellExperiment)
library(scp)
library(scpdata)
library(limma)
## Utility packages for data manipulation and visualization
library(tidyverse)
library(patchwork)
```

### `scpdata` and the `leduc2022` dataset

We also implemented a data package called `scpdata`. It distributes
published SCP datasets, such as the `leduc2022` dataset. The datasets
were downloaded from the data source provided in the publication and
formatted to a `QFeatures` object so that it is compatible with our
software. The underlying data storage is based on the `ExperimentHub`
package that provides a cloud-based storage infrastructure.

The `leduc2022` dataset is provided at different levels of
processing:

* The **raw data** files that were generated by the mass-spectrometer
software. This data is not included in `scpdata`. 
* A **PSM data** table obtained from the MaxQuant software that
performs spectrum identification and quantification. PSM stands for 
*peptide to spectrum match* where MS spectra could 
successfully be assigned to peptide sequences. The provided PSM 
table was further processed using DART-ID (\@Chen2019-uc) to improve
the identification rate.
* **Peptide data** are provided as intermediate output tables from the
data processing. There are 2 tables: peptides normalized by reference 
and peptides further normalized (median centering of columns and 
rows) and log-transformed.
* A **protein data** are provided as intermediate and final output 
tables from the data processing. There are 2 tables: proteins column
and row centered to median and log2 transformed and proteins after
imputation, batch correction and final column and row centering.

The workflow starts with the PSM table and will generate the peptide
and the protein data. The authors provided the PSM dataset as a tabular 
text file called `ev_updated.txt`. Peptide and protein data are shared
as CSV files. We highly value the effort the authors have made to 
publicly share all the data generated in their project, from raw files
to final expression tables (see the Slavov Lab 
[website](https://scp.slavovlab.net/Leduc_et_al_2022)).

We formatted the `leduc2022` dataset following our data framework. The 
formatted data can be retrieved from the `scpdata` package using the 
`leduc2022()` function. All datasets in `scpdata` are called after 
the first author and the date of publication.

```{r load_data, eval=FALSE}
scp <- leduc2022()
```

## To include in `scpdata`

Actually, `leduc2022()` is still in development and is not yet 
available from `scpdata`. The code below will soon be published in
`scpdata`. We here include it so that we can still run the vignette. 

```{r}
# All files were downloaded from 
# https://drive.google.com/drive/folders/117ZUG5aFIJt0vrqIxpKXQJorNtekO-BV

datadir <- "~/PhD/.localdata/SCP/leduc2022/"

####---- Prepare sample annotations ----####

# The sample annotations are provided in 2 separate tables:
design <- read.csv(paste0(datadir, "annotation.csv"))
batch <- read.csv(paste0(datadir, "batch.csv"))

# Clean the sample metadata so that it meets the requirements for
# `scp::readSCP`. We first need to transform the design (set x 
# reporter ion) to a long table so that one line is one sample. 
design <- pivot_longer(design, -Set, names_to = "Channel", 
                       values_to = "SampleAnnotation")
design$SampleType <- recode(design$SampleAnnotation, 
                            neg = "NegControl",
                            u = "Monocyte",
                            m = "Melanoma cell",
                            unused = "Unused",
                            reference = "Reference",
                            carrier = "Carrier")

# We then make some slight corrections to the batch data
colnames(batch)[1] <- "Set" ## consistent naming with design
batch$digest <- as.character(batch$digest)

# We can now combine the two tables in a single annotation table
sampleAnnotation <- inner_join(design, batch, by = "Set")

####---- Prepare PSM data ----####

ev <- read.delim(paste0(datadir, "ev_updated.txt"))
colnames(ev) <- gsub("^Reporter.intensity.(\\d*)$", "RI\\1", colnames(ev))
colnames(ev)[colnames(ev) == "Raw.file"] <- "Set"
ev$modseq <- paste0(ev$Modified.sequence, ev$Charge)
## This removes DIA runs
ev <- ev[ev$Set %in% sampleAnnotation$Set, ]

## Create the QFeatures object
leduc <- readSCP(ev, sampleAnnotation, 
                 channelCol = "Channel", 
                 batchCol = "Set")

## Clean protein names
rdList <- lapply(rowData(leduc), function (rd) {
    rd$Leading.razor.protein.cleaned <- 
        gsub("^.*\\|(.*)\\|.*", "\\1", rd$Leading.razor.protein)
    rd
})
rowData(leduc) <- rdList

####---- Retrieve processed data ----####

## Retrieve the data processed by Leduc et al. 
sampleInd <- read.csv(paste0(datadir, "misc/sample_index.csv"), row.names = 2)
files <- c("t0.csv", "t3.csv", "t4b.csv", "t6.csv")
processedData <- lapply(files, function(f) {
    ## Read data
    dat <- read.csv(paste0(datadir, "processed_data/", f), row.names = 1)
    dat <- as.matrix(dat)
    ## Convert column names
    fileID <- sub("X", "", sampleInd[colnames(dat), "rawfile"])
    channel <- sampleInd[colnames(dat), "channel"]
    colnames(dat) <- paste0(fileID, "RI", channel)
    ## Convert to a SCE
    dat <- SingleCellExperiment(dat)
    ## Add colData
    colData(dat) <- colData(leduc)[colnames(dat), ]
    dat
})
names(processedData) <- c("peptides", "peptides_log", "proteins_norm2", "proteins_processed") 

## Add `peptides` data
leduc <- addAssay(leduc, processedData$peptides, name = "peptides")
## TODO: add AssayLinks

## Add `peptides_log` data
leduc <- addAssay(leduc, processedData$peptides_log, name = "peptides_log")
## TODO: add AssayLinks

## Add `proteins_norm2` data
leduc <- addAssay(leduc, processedData$proteins_norm2, name = "proteins_norm2")
## TODO: add AssayLinks

## Add `proteins_processed` data
leduc <- addAssay(leduc, processedData$proteins_processed, name = "proteins_processed")
## TODO: add AssayLinks
```

**Stop "To include in `scpdata`"**


The data contain 138 different `SingleCellExperiment` objects that we
refer to as **assays**. Each assay contains expression data along with
feature metadata. Each row in an assay represents a **feature** that 
can either be a PSM, a peptide or a protein depending on the assay. Each 
column in an assay represents a **sample**. In the `leduc` object, 
samples are pooled using TMT-pro18 labeling, hence each assay contains
18 columns. Most samples are single-cells, but some samples are negative
controls, references, carriers,... Below, we show the overview of the 
`leduc` object

```{r overview}
leduc
```

134 out of the 138 assays are PSM data, each assay corresponding to a
separate MS run. Notice that the assays were acquired in 2 sample 
preparation and chromatographic batches. 

```{r}
table(LcBatch = leduc$lcbatch,
      SamplePrepBatch = sub("AL.*", "", leduc$Set))
```

The dataset also contains a `peptides`, `peptides_log`, `proteins_norm`
and `proteins_processed` assay. Those were provided by the authors. 
The objective of this vignette is to replicate these assays from the 
134 PSM assays following the same procedure as the original script but
using standardized functionality.

We extract these latter assays and keep them for
later benchmarking. Using double brackets `[[...]]` extracts the
desired assay as a `SingleCellExperiment` object. On the other hand,
using simple brackets `[row, col, assay]` subsets the desired
elements/assays but preserves the `QFeatures` data structure.

```{r extract_SCoPE2_assays}
peptides_leduc <- leduc[["peptides"]]
peptides_log_leduc <- leduc[["peptides_log"]]
proteins_norm_leduc <- leduc[["proteins_norm2"]]
proteins_processed_leduc <- leduc[["proteins_processed"]]
leduc <- leduc[, , -(135:138)]
```

We will compare the replications by comparing the set of filtered 
features (peptides or proteins) and samples. This is performed using 
this function.

```{r}
compareSets <- function(setleduc, setscp) {
    allElements <- unique(c(setleduc, setscp))
    table(leduc2022 = allElements %in% setleduc,
          scp = allElements %in% setscp)
}
```

We will also compare the replication based on the quantitative data. 
We again create a dedicated function to perform this.

```{r}
compareQuantitativeData <- function(sceleduc, scescp) {
    rows <- intersect(rownames(sceleduc),
                      rownames(scescp))
    cols <- intersect(colnames(sceleduc),
                      colnames(scescp))
    err <- assay(sceleduc)[rows, cols] - assay(scescp)[rows, cols]
    data.frame(difference = as.vector(err[!is.na(err)])) %>%
        ggplot() +
        aes(x = difference) +
        geom_histogram(bins = 50) +
        xlab("nPOP - scp") +
        scale_y_continuous(labels = scales::scientific) +
        theme_minimal()
}
```

# PSM filtering

After importing the data, Leduc et al. filter low-confidence PSMs. 
Each PSM assay contains feature meta-information that are stored
in the assay `rowData`. The `QFeatures` package allows to quickly filter
the rows of an assay by using these information. The available
variables in the `rowData` are listed below for each assay.

```{r rowDataNames}
rowDataNames(leduc)
```

## Remove contaminant, noisy and low-confidence spectra

We first remove spectra that are matched to contaminant proteins and 
reverse hits. We also remove PSMs that have been matched from impure
spectra, that are spectra containing co-eluting peptides. These are 
identified based on the parental ion fraction (PIF), computed by 
MaxQuant. Finally, we also want to remove PSM with poor matching 
confidence, as defined by the false discovery rate (FDR) computed by 
DART-ID. 

We can extract the information from the `rowData` of several assays using
the `rbindRowData` function. It takes the `rowData` of interest and
returns a single `DataFrame` table with variables of interest. We 
extract such a table for the different variables listed above to 
create a quality control plot.

```{r qvalue_plot, message=FALSE}
rd <- data.frame(rbindRowData(leduc, i = names(leduc)))
ggplot(rd) +
    aes(x = dart_PEP) +
    geom_histogram() +
    geom_vline(xintercept = 0.01) +
    ggplot(rd) +
    aes(x = PIF) +
    geom_histogram() +
    geom_vline(xintercept = 0.6)
```

We next remove the PSMs that are matched to potential contaminants 
(`Potential.contaminant` is `+` and `Proteins` starts with `CON`), 
reverse hits (`Reverse` is `+` and `Leading.razor.protein` starts with
`REV`), noisy spectra (`PIF` is missing or greater than 0.6) and low-confidence
spectra with at 1% FDR threshold (`dart_qval` smaller than 0.01). We
can perform this on our `QFeatures` object using the `filterFeatures()`
function. The different pieces of information are directly accessed
from the `rowData` of each assay.

```{r}
leduc <- filterFeatures(leduc, ~ Potential.contaminant != "+" &
                            !grepl("CON", Proteins) &
                            Reverse != "+" &
                            !grepl("REV", Leading.razor.protein) &
                            (is.na(PIF) | PIF > 0.6) &
                            dart_qval < 0.01)
```

**!discussion**:

- keeping PIF that is missing seems strange... Replacing NAs by 0.6 
looks odd. This is seen in the original code. 

```{r, eval=FALSE}
ev <- ev %>% filter(PIF >.6 | is.na(PIF)==T)
ev$PIF[is.na(ev$PIF)==T] <- .6
```

- This workflow is missing a step because we cannot currently filter based
on the quantitative data using the `QFeatures`/`scp` framework. However,
this filter is implied when computing the SCR since a zero carrier will
not allow to compute SCR. The missing step is:

```{r, eval=FALSE}
ev <- ev %>% filter(Reporter.intensity.corrected.1 != 0)
```

## Sample to carrier filter

The PSMs are next filtered based on the sample to carrier ratio (SCR),
that is the TMT ion intensity of a single-cell sample divided by
the TMT ion intensity of the carrier (200 cell equivalent) acquired during
the same run as the sample. It is expected that the carrier
intensities are much higher than the single-cell intensities. We
implemented the `computeSCR()` function that computes the SCR for each
PSM averaged over all samples of interest in a given assay. A PSM is
removed when the mean SCR exceeds 10 \%. To perform this, we need to
tell the function which columns are the samples of interest and which
column is the carrier. The `colData` of the `QFeatures` object is used 
to define this.

```{r}
table(leduc$SampleType)
```

In this dataset, `SampleType` gives the type of sample that is present
in each TMT channel. There 5 types of samples:

- The carrier channels (`Carrier`) contain 200 cell equivalents and
are meant to boost the peptide identification rate.
- The normalization channels (`Reference`) are used to partially 
correct for between-run variation.
- The unused channels (`Unused`) are channels that are left empty due
to isotopic cross-contamination.
- The negative controls (`NegControl`) contain samples that do not contain any
cell but are processed as single-cell samples.
- The single-cell sample channels contain the single-cell samples of
interest (`Melanoma cell` or `Monocyte`).

The `computeSCR` function expects the user to provide a pattern
(following regular expression syntax) that uniquely identifies a
carrier channel in each run and the samples or blanks. The function
will store the mean SCR of each feature in the `rowData` of each
assay.

```{r}
leduc <- computeSCR(leduc, names(leduc),
                    colvar = "SampleType", 
                    samplePattern = "Mel|Macro|Neg",
                    carrierPattern = "Carrier",
                    sampleFUN = "mean",
                    rowDataName = "MeanSCR")
```

**!discussion**: It is probably better to exclude negative controls 
when computing the SCR. Since it should not heavily influence the 
results, we replicate the original study. 

Before applying the filter, we plot the distribution of the mean SCR.

```{r message=FALSE, warning=FALSE}
rbindRowData(leduc, i = names(leduc)) %>%
    data.frame %>%
    ggplot(aes(x = MeanSCR)) +
    geom_histogram() +
    geom_vline(xintercept = 0.1) +
    scale_x_log10()
```

A great majority of the PSMs have a mean SCR that is lower than 10\%,
as expected. Since the mean SCR is stored in the `rowData`, we can 
apply `filterFeatures()` on the object to remove PSMs with high 
average SCR.

```{r}
leduc <- filterFeatures(leduc, ~ 
                            !is.na(MeanSCR) & !is.infinite(MeanSCR) &
                            MeanSCR < 0.05)
```

## Filter on summed single-cell signal

Finally, we remove PSM that have no signal in single-cell samples. This
is not explicitely implemented in `scp`. To add custom information to
`rowData`, you need to provide a list of `DataFrame`s. The name of the
elements in the list should correspond to the names of the assays where
the `rowData` is modified. The column names of the `DataFrame` indicate
which variable should be modified or added (if they do not exist yet).
So, for each assay, we compute the summed signal in single-cells (and
negative controls) and store the results in a `DataFrame`.

```{r}
sums <- lapply(names(leduc), function(i) {
    sce <- leduc[[i]]
    sel <- grep("Mel|Macro|Neg", colData(leduc)[colnames(sce), "SampleType"])
    x <- assay(sce)[, sel, drop = FALSE]
    rs <- rowSums(x, na.rm = TRUE)
    DataFrame(ScSums = rs)
})
```

**!discussion**: is this chunk above difficult to read? I need to 
discuss this with Laurent. 

The list of `DataFrame` is named after the corresponding assays and 
the `rowData` of the `leduc` object is modified.

```{r}
names(sums) <- names(leduc)
rowData(leduc) <- sums
```

To verify this new piece of data was correctly added, we plot the summed
signal for each PSM. 

```{r warning=FALSE, message=FALSE}
rbindRowData(leduc, i = names(leduc)) %>%
    data.frame %>%
    ggplot(aes(x = ScSums)) +
    geom_histogram() +
    scale_x_log10()
```

We apply the final filter using `filterFeatures()`. 

```{r}
leduc <- filterFeatures(leduc, ~ ScSums != 0)
```

# Normalize to reference

In order to partially correct for between-run variation, Leduc et al.
compute relative reporter ion intensities. This means that
intensities measured for single-cells are divided by the reference
channel. We use the `divideByReference()` function that divides 
channels of interest by the reference channel.
Similarly to `computeSCR`, we can point to the samples and the
reference columns in each assay using the annotation contained in the
`colData`. We will here divide all columns (using the regular
expression wildcard `.`) by the reference channel (`Reference`).

```{r}
leduc <- divideByReference(leduc, i = names(leduc),
                           colvar = "SampleType", 
                           samplePattern = ".",
                           refPattern = "Reference")
```

**!discussion**: Notice that when taking all samples we also include the reference
channel itself. Hence, from now on, the reference channels will 
contain only ones.

# Aggregate PSM data to peptide data

Now that the PSM assays are processed, we can aggregate them to
peptides. This is performed using the `aggregateFeaturesOverAssays()`
function. This is a wrapper function in `scp` that sequentially calls
the `aggregateFeatures` from the `QFeatures` package over the
different assays. For each assay, the function aggregates several PSMs
into a unique peptide given an aggregating variable in the `rowData`
(peptide sequence) and a user-supplied aggregating function (the
median for instance). Regarding the aggregating function, the original
analysis removes duplicated peptide sequences per run by taking the
first non-missing value. While better alternatives are documented in
`QFeatures::aggregateFeatures`, we still use this approach for the
sake of replication and for illustrating that custom functions can
be applied.

```{r remove.duplicates}
remove.duplicates <- function(x)
    apply(x, 2, function(xx) xx[which(!is.na(xx))[1]] )
```

The aggregated peptide assays must be given a name. We here used the
original names with `peptides_` at the start.

```{r}
peptideAssays <- paste0("peptides_", names(leduc))
```

We now have all the required information to aggregate the PSMs in the
different batches to peptides.

```{r}
leduc <- aggregateFeaturesOverAssays(leduc,
                                     i = names(leduc),
                                     fcol = "modseq",
                                     name = peptideAssays,
                                     fun = remove.duplicates)
```

Under the hood, the `QFeatures` architecture preserves the
relationship between the aggregated assays. See `?AssayLinks` for more
information on relationships between assays. Notice that
`aggregateFeaturesOverAssays` created as many new assays as the number
of supplied assays.

```{r}
leduc
```

# Join assays 

Up to now, we kept the data belonging to each MS run in separate
assays. We now combine all batches into a single assay. This can
easily be done using the `joinAssays()` function from the `QFeatures`
package.

## Consensus mapping of peptides to proteins

We need to account for an issue in the data. `joinAssays()` will only
keep the metadata variables that have the same value between matching
rows. However, some peptide sequences map to one protein in one run
and to another protein in another run. Hence, the protein sequence is
not constant for all peptides and is removed during joining. It is
important we keep the protein sequence in the `rowData` since we will
later need it to aggregate peptides to proteins. To avoid this issue,
we replace the problematic peptides to protein mappings through a
majority vote.

```{r}
## Generate a list of DataFrames with the information to modify
rbindRowData(leduc, i = grep("^pep", names(leduc))) %>%
    data.frame %>%
    group_by(modseq) %>%
    ## The majority vote happens here
    mutate(Leading.razor.protein.cleaned =
               names(sort(table(Leading.razor.protein.cleaned),
                          decreasing = TRUE))[1]) %>%
    select(modseq, Leading.razor.protein.cleaned) %>%
    filter(!duplicated(modseq, Leading.razor.protein.cleaned)) ->
    ppMap
consensus <- lapply(peptideAssays, function(i) {
    ind <- match(rowData(leduc[[i]])$modseq, ppMap$modseq)
    DataFrame(Leading.razor.protein.cleaned =
                  ppMap$Leading.razor.protein.cleaned[ind])
})
## Name the list
names(consensus) <- peptideAssays
## Modify the rowData
rowData(leduc) <- consensus
```

## Cleaning missing data

Another important step before we join the assays is to replace zero 
and infinite values by `NA`s. The zeros can be biological zeros or 
technical zeros and differentiating between the two types is a 
difficult task, they are therefore better considered as missing. The 
infinite values arose during the normalization by the reference because
the channel values are divide by a zero from the reference channel.
This artefact could easily be avoided if we had replace the zeros by
`NA`s at the beginning of the workflow, what we strongly recommend for
future analyses.

The `infIsNA()` and the `zeroIsNA()` functions automatically detect
infinite and zero values, respectively, and replace them with `NA`s.
Those two functions are provided by the `QFeatures` package.

```{r}
leduc <- infIsNA(leduc, i = peptideAssays)
leduc <- zeroIsNA(leduc, i = peptideAssays)
```

## Join assays

Now that the peptides are correctly matched to proteins and missing
values are correctly formatted, we can join the assays.

```{r join_assays}
leduc <- joinAssays(leduc,
                    i = peptideAssays,
                    name = "peptides")
```

`joinAssays` has created a new assay called `peptides` that combines
the previously aggregated peptide assays.

```{r overview3}
leduc
```

# Filter single-cells based on median CV

Leduc et al. proceed with filtering the single-cells. The
filtering is mainly based on the median coefficient of variation (CV)
per cell. The median CV measures the consistency of quantification for
a group of peptides that belong to a protein. We remove cells that
exhibit high median CV over the different proteins. We compute the
median CV per cell using the `medianCVperCell()` function from the `scp`
package. The function takes the protein information from the `rowData`
of the assays that will tell how to group the features (peptides) when
computing the CV. Note that we supply the peptide assays before
joining in a single assays (`i = peptideAssays`). This is because
SCoPE2 performs a custom normalization (`norm = "SCoPE2"`). Each row
in an assay is normalized by a scaling factor. This scaling factor is
the row mean after dividing the columns by the median. The authors
retained CVs that are computed using at least 3 peptides (`nobs = 3`).

```{r compute_medianCV}
leduc <- medianCVperCell(leduc,
                         i = peptideAssays,
                         groupBy = "Leading.razor.protein.cleaned",
                         nobs = 3,
                         na.rm = TRUE,
                         colDataName = "MedianCV",
                         norm = "SCoPE2")
```

The computed CVs are stored in the `colData`. We can now filter cells
that have reliable quantifications. The negative controls are not expected
to have reliable quantifications and hence can be used to estimate a
null distribution of the CV. This distribution helps defining a
threshold that filters out single-cells that contain noisy 
quantification.

```{r}
colData(leduc) %>% 
    data.frame %>% 
    filter(grepl("Mono|Mel|Neg", SampleType)) %>% 
    mutate(control = ifelse(grepl("Neg", SampleType), "ctl", "sc")) %>% 
    ggplot() +
    aes(x = MedianCV,
        fill = control) + 
    geom_density(alpha = 0.5, adjust = 1) +
    geom_vline(xintercept = 0.42) +
    xlim(0.2, 0.8) +
    theme_minimal() +
    scale_fill_manual(values = c( "black", "purple2")) + 
    xlab("Quantification variability") + 
    ylab("Fraction of cells")
```

We can see that the protein quantification for single-cells are much 
more consistent within single-cell channels than within blank channels.
A threshold of 0.42 best separates single-cells from empty channels.

We keep the cells that pass the median CV threshold. Furthermore, we
keep melanoma cells and monocytes as those represent the samples of
interest. We can extract the sample names that pass the CV and sample
type filters using the `subsetByColData()` function. 

```{r}
leduc <- 
    subsetByColData(leduc,
                    !is.na(leduc$MedianCV) &
                        leduc$MedianCV < 0.42 &
                        grepl("Mono|Mel", leduc$SampleType))
```

# Compare intermediate results

At this stage of the processing, the last assay should be similar to
the `peptides_leduc` data provided by the authors. Let's compare the
filtered cells. 

```{r}
compareSets(colnames(peptides_leduc), 
            colnames(leduc[["peptides"]]))
```

There is an excellent agreement between the the original and the
replicated vignette. Let's do the same for the filtered peptides. 

```{r}
compareSets(rownames(peptides_leduc), 
            rownames(leduc[["peptides"]]))
```

Finally let's compare the quantitative data 

```{r}
compareQuantitativeData(peptides_leduc, leduc[["peptides"]])
```

The replication is close to perfect. Note however that this 
vignette is more stringent with respect to the number of selected 
peptides. We cannot explain this difference.  

# Normalization

The columns (samples) then the rows (peptides) are normalized by 
dividing the relative intensities by the median relative intensities.
The column normalization is implemented as the `normalize()` function
with the argument `method = div.median`. The row normalization is not
available from `normalizet()`, but is easily performed using the 
`sweep` function from the `QFeatures` package that is inspired from 
the `base::sweep` function. 

```{r}
## Scale column with median
leduc <- normalize(leduc,
                   i = "peptides",
                   method = "div.median",
                   name = "peptides_norm1")
## Scale rows with median
leduc <- sweep(leduc,
               i = "peptides_norm1",
               name = "peptides_norm2",
               MARGIN = 1,
               FUN = "/",
               STATS = rowMedians(assay(leduc[["peptides_norm1"]]),
                                  na.rm = TRUE))
```

Each normalization step is stored in a separate assay. An important
aspect to note here is that

# Missing data filtering

Peptides that contain many missing values are not informative. 
Therefore, the authors remove those with more than 99 \% missing data.
This is done using the `filterNA()` function from `QFeatures`.

```{r}
leduc <- filterNA(leduc,
                  i = "peptides_norm2",
                  pNA = 0.99)
```

They also remove cells with more than 99 \% missing data. This is 
performed by first computing the amount of missing data in the assay
using `nNA()`. We then subset the cells that meet the criterion. 

```{r}
nnaRes <- nNA(leduc, "peptides_norm2")
sel <- nnaRes$nNAcols$pNA < 99
leduc[["peptides_norm2"]] <- leduc[["peptides_norm2"]][, sel]
```

# Log-transformation

Peptide data is log2-transformed before aggregating to proteins. This
is performed by the `logTransform()` function from `QFeatures`.

```{r}
leduc <- logTransform(leduc,
                      base = 2,
                      i = "peptides_norm2",
                      name = "peptides_log")
```

# Compare intermediate results

At this stage of the processing, the last assay should be similar to
the `peptides_log_leduc` data provided by the authors. Let's compare the
filtered cells. 

```{r}
compareSets(colnames(peptides_log_leduc), 
            colnames(leduc[["peptides_log"]]))
```

There is an excellent agreement between the the original and the
replicated vignette. Let's do the same for the filtered peptides. 

```{r}
compareSets(rownames(peptides_log_leduc), 
            rownames(leduc[["peptides_log"]]))
```

Notice here that most peptides that this vignette removed earlier are
now also removed by the original analysis. There is an excellent
agreement as well between selected peptides. Finally let's compare the
quantitative data.

```{r}
compareQuantitativeData(peptides_log_leduc, leduc[["peptides_log"]])
```

The agreement is still very good, with a sharp peak around 0. However, 
we can see that the range of differences starts to increase, probably
because numerical differences propagate as we progress through the 
data processing. 

# Aggregate peptide data to protein data

Similarly to aggregating PSM data to peptide data, we can aggregate 
peptide data to protein data using the `aggregateFeatures` function. 
Note that we here use the median as a summarizing function.

```{r, message=FALSE, warning=FALSE}
leduc <- aggregateFeatures(leduc,
                           i = "peptides_log",
                           name = "proteins",
                           fcol = "Leading.razor.protein.cleaned",
                           fun = matrixStats::colMedians, 
                           na.rm = TRUE)
```

# Normalization

Normalization is performed similarly to peptide normalization. We use
the same functions, but since the data were log-transformed at the
peptide level, we subtract by the median instead of dividing.

```{r}
## Center columns with median
leduc <- normalize(leduc,
                   i = "proteins",
                   method = "center.median",
                   name = "proteins_norm1")
## Scale rows with median
leduc <- sweep(leduc,
               i = "proteins_norm1",
               name = "proteins_norm2",
               MARGIN = 1,
               FUN = "-",
               STATS = rowMedians(assay(leduc[["proteins_norm1"]]),
                                  na.rm = TRUE))
```


# Compare intermediate results

At this stage of the processing, the last assay should be similar to
the `proteins_norm_leduc` data provided by the authors. Let's compare the
filtered cells. 

```{r}
compareSets(colnames(proteins_norm_leduc), 
            colnames(leduc[["proteins_norm2"]]))
```

There is an excellent agreement between the the original and the
replicated vignette. Let's do the same for the filtered proteins. 

```{r}
compareSets(rownames(proteins_norm_leduc), 
            rownames(leduc[["proteins_norm2"]]))
```

There is an almost perfect agreement between the selected proteins.
selected Finally let's compare the quantitative data.

```{r}
compareQuantitativeData(proteins_norm_leduc, leduc[["proteins_norm2"]])
```

Again, there is a very sharp peak around 0. 

# Imputation

The protein data is majorily composed of missing values. The graph below shows the
distribution of the proportion missingness in cells. Cells contain on average
65 \% missing values. 

```{r missing_plot, message=FALSE}
data.frame(pNA = nNA(leduc, "proteins_norm2")$nNAcols$pNA) %>%
    ggplot(aes(x = pNA)) +
    geom_histogram() +
    xlab("Percentage missingnes per cell")
```

**!discussion**: given this graph above, should cells not better be 
filtered with 80\% missing values instead of 99\%?

The missing data is imputed using K nearest neighbors. The authors run
KNN with k = 3. We made a wrapper around the author's code to apply
imputation to our `QFeatures` object.

```{r}
leduc <- imputeKnnSCoPE2(leduc,
                         i = "proteins_norm2",
                         name = "proteins_impd",
                         k = 3)
```

`QFeatures` provides the `impute` function that serves as an interface
to different imputation algorithms among which the KNN algorithm from
`impute::impute.knn`. However, the KNN implementation in the oringal analysis and in 
`impute.knn` are different.  Leduc et al. perform KNN imputation in the 
sample space, meaning that data from neighbouring cells are used to 
impute the central cell, whereas `impute::impute.knn` performs KNN 
imputation in the feature space, meaning that data from neighbouring 
features are used to impute the missing values from the central 
features. We provide the code for KNN imputation with `QFeatures` but
do not run in order to replicate the original analysis.

```{r, eval = FALSE}
leduc <- impute(leduc,
                i = "proteins_norm2",
                method = "knn",
                k = 3, rowmax = 1, colmax= 1,
                maxp = Inf, rng.seed = 1234)
```

# Batch correction

The next step is to correct for the remaining batch effects.
The data were acquired as a series of MS runs. Recall we had 134
assays at the beginning of the workflow. Each MS run can be subjected
to technical perturbations that lead to differences in the data. Furthermore, 
TMT labeling can also influence the quantification. These effects
must be accounted for to avoid attributing biological effects to 
technical effects. The `limma` algorithm (CITE-Ritchie) is used by 
Leduc et al. to correct for batch effects. It can take up to 2
batch variables, in this case the  MS acquisition batch and the TMT channel,
while protecting for variables of interest, the sample type in this case. All the 
information is contained in the `colData` of the `QFeatures` object.
We first extract the assays with the associated `colData`.

```{r}
sce <- getWithColData(leduc, "proteins_impd")
```

We next create the design matrix. We then perform the batch correction
and overwrite the data matrix. Recall the 
data matrix can be accessed using the `assay` function. 

```{r}
model <- model.matrix(~ SampleType, data = colData(sce))
assay(sce) <- removeBatchEffect(x = assay(sce),
                                batch = sce$lcbatch,
                                batch2 = sce$Channel,
                                design = model)
```

Finally, we add the batch corrected assay to the `QFeatures` object
and create the feature links.

```{r}
leduc <- addAssay(leduc, y = sce, name = "proteins_batchC")
leduc <- addAssayLinkOneToOne(leduc, from = "proteins_impd",
                              to = "proteins_batchC")
```

# Normalization 

The very last step of the data processing workflow is a new round of
normalization.

```{r}
## Center columns with median
leduc <- normalize(leduc,
                   i = "proteins_batchC",
                   method = "center.median",
                   name = "proteins_batchC_norm1")
## Scale rows with median
leduc <- sweep(leduc,
               i = "proteins_batchC_norm1",
               name = "proteins_processed",
               MARGIN = 1,
               FUN = "-",
               STATS = rowMedians(assay(leduc[["proteins_batchC_norm1"]]),
                                  na.rm = TRUE))
```

**!discussion**: non imputed data is considered as the final protein
data where values that were missing before imputation are replaced by
zero, this does not seem correct.



# Compare the final results

At the end of the processing, the last assay should be similar to
the `proteins_processed_leduc` data provided by the authors. Let's compare the
filtered cells. 

```{r}
compareSets(colnames(proteins_processed_leduc), 
            colnames(leduc[["proteins_processed"]]))
```

There is an excellent agreement between the selected cells from the 
original and this vignette. Let's do the same for the filtered 
proteins. 

```{r}
compareSets(rownames(proteins_processed_leduc), 
            rownames(leduc[["proteins_processed"]]))
```

The agreement is also excellent between filtered proteins. 
Finally let's compare the quantitative data 

```{r}
compareQuantitativeData(proteins_processed_leduc, 
                        leduc[["proteins_processed"]])
```

The differences are still sharply peaked around 0. However the differences
are more spread and the range is larger compared to the previous steps. 

Overall, we can see good replication of the data processing, although
early differences seem to get amplified as we progress through the 
different processing steps. We next compare the dimension reduction
results to get a more qualitative assessment. 

# PCA 

We run the same PCA procedure as performed by the authors, that is a
weighted PCA where the weight for a protein is defined as the summed
correlation with the other proteins.

```{r}
sce <- getWithColData(leduc, "proteins_processed")
pcaRes <- pcaSCoPE2(sce)
## Compute percent explained variance
pcaPercentVar <- round(pcaRes$values[1:2] / sum(pcaRes$values) * 100)
## Plot PCA
data.frame(PC = pcaRes$vectors[, 1:2],
           colData(sce)) %>%
    ggplot() +
    aes(x = PC.1, 
        y = PC.2, 
        colour = SampleType) +
    geom_point(alpha = 0.5) +
    xlab(paste0("PC1 (", pcaPercentVar[1], "%)")) +
    ylab(paste0("PC2 (", pcaPercentVar[2], "%)"))+
    ggtitle("PCA on scp processed protein data")
```

The PCA plot is very similar to the published PCA plot.

```{r}
pcaResLeduc <- pcaSCoPE2(proteins_processed_leduc)
## Compute percent explained variance
pcaPercentVar <- round(pcaResLeduc$values[1:2] / sum(pcaResLeduc$values) * 100)
## Plot PCA
data.frame(PC = pcaResLeduc$vectors[, 1:2],
           colData(proteins_processed_leduc)) %>%
    ggplot() +
    aes(x = PC.1, 
        y = PC.2, 
        colour = SampleType) +
    geom_point(alpha = 0.5) +
    xlab(paste0("PC1 (", pcaPercentVar[1], "%)")) +
    ylab(paste0("PC2 (", pcaPercentVar[2], "%)")) +
    ggtitle("PCA on processed protein data by Leduc et al.")
```

Here again we can see the replicated PCA from this vignette is very 
similar to the PCA published by the authors. 

Using standard PCA, we obtain the same cell patterns although the 
explained variance differs. 

```{r}
library(scater)
## Perform PCA, see ?runPCA for more info about arguments
runPCA(sce, ncomponents = 50,
       ntop = Inf,
       scale = TRUE,
       exprs_values = 1,
       name = "PCA") %>%
    ## Plotting is performed in a single line of code
    plotPCA(colour_by = "SampleType")
```

# Conclusion 

In this vignette, we have demonstrated that the `scp` package is able
to accurately reproduce the analysis published by Leduc et al. We not only 
support the reliability of the published work, but we also offer a
formalization and standardization of the pipeline by means of 
easy-to-read and highly documented code. This workflow can serve as a
starting ground to improve upon the current methods and to design new
modelling tools dedicated to single-cell proteomics. 

# Reproduce this vignette

You can reproduce this vignette using `Docker`:

```
docker pull cvanderaa/scp_replication_docker:v1
docker run \
    -e PASSWORD=bioc \
    -p 8787:8787 \
    cvanderaa/scp_replication_docker:v1
```

Open your browser and go to http://localhost:8787. The USER is `rstudio` and 
the password is `bioc`. You can find the vignette in the `vignettes` folder. 

See the 
[website home page](https://uclouvain-cbio.github.io/SCP.replication/index.html)
for more information.

# Requirements

### Hardware and software

The system details of the machine that built the vignette are:

```{r, echo = FALSE, message = FALSE}
sd <- benchmarkme::get_sys_details()
cat("Machine: ", sd$sys_info$sysname, " (", sd$sys_info$release, ")\n",
    "R version: R.", sd$r_version$major, ".", sd$r_version$minor,
    " (svn: ", sd$r_version$`svn rev`, ")\n",
    "RAM: ", round(sd$ram / 1E9, 1), " GB\n",
    "CPU: ", sd$cpu$no_of_cores, " core(s) - ", sd$cpu$model_name, "\n",
    sep = "")
```

### Timing

The total time required to compile this vignette is:

```{r, echo = FALSE}
timing <- Sys.time() - timeStart
cat(timing[[1]], attr(timing, "units"))
```
### Memory

The final `leduc` object size is:

```{r, echo = FALSE}
format(object.size(leduc), units = "GB")
```
### Session info

```{r}
sessionInfo()
```

# Licence

This vignette is distributed under a 
[CC BY-SA licence](https://creativecommons.org/licenses/by-sa/2.0/) 
licence.

# Reference